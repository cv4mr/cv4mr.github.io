<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie9 lt-ie8" lang="en"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie9" lang="en"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"> <!--<![endif]-->
<head>
<title>CV4MR</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<link rel="stylesheet" href="stylesheets/foundation.min.css">
<link rel="stylesheet" href="stylesheets/main.css">
<link rel="stylesheet" href="stylesheets/app.css">
<script src="javascripts/modernizr.foundation.js"></script>
<!-- Google fonts -->
<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300|Playfair+Display:400italic' rel='stylesheet' type='text/css' />
<!-- IE Fix for HTML5 Tags -->
<!--[if lt IE 9]>
<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>
<body>

<h1 style="text-align:center;white-space:pre-wrap;"><b>CV4MR</b></h1><h2 style="text-align:center;white-space:pre-wrap;"><b>Workshop on Computer Vision for Mixed Reality</b></h2><h3 style="text-align:center;white-space:pre-wrap;">in conjunction with CVPR 2023</h3><p style="text-align:center;white-space:pre-wrap;" class=""></p>

<div class="row page_wrap" style="margin-top:-2px">
  <!-- page wrap -->
  <div class="twelve columns">
    <!-- page wrap -->
    <div class="row">
      <div class="twelve columns header_nav" style="margin-bottom:0; box-shadow: none">
        <div class="twelve columns">
          <ul id="menu-header" class="nav-bar horizontal">
            <li><a href="index.html">Home</a></li>
            <li><a href="#overview">Overview</a></li>
            <li><a href="#cfp">Call for Papers</a></li>
            <li><a href="#speakers">Speakers</a></li>
            <li><a href="#organizers">Organizers</a></li>
          </ul>
          <script>$('ul#menu-header').nav-bar();</script>
        </div>
      </div>
    </div>
    <!-- END Header -->
    <div class="row">
      <div class="blog_post">
        <!-- Begin Blog Post -->
        <!-- ############################## section begin -->
        <div class="twelve columns" style="margin-bottom: 25px" id="overview">
          <div class="heading_dots_grey  hide-for-smal">
            <h1 class="heading_largesize" style="margin-bottom:0"><span class="heading_bg"> OVERVIEW </span></h1>
          </div>
        </div>
        <div>
          <p>VR technologies have the potential to transform the way we use computing to interact with our environment, do our work and connect with each other. VR devices provide users with immersive experiences at the cost of blocking the visibility of the surrounding environment. With the advent of passthrough techniques such as those in Quest Pro, now users can build deeply immersive experiences which mix the virtual and the real world into one, often also called Mixed Reality. While there have been workshops focused on CV for VR in past CVPR (cv4arvr), Mixed Reality poses a set of very unique research problems in computer vision that are not covered by VR. Our focus is on capturing the real environment around the user using cameras which are placed away from the user’s eyes, yet reconstruct the environment with high fidelity, augmented the environment with virtual objects and effects, and all in real-time. This would offer the research community to deeply understand the unique challenges of Mixed Reality and research on novel methods encompassing View Synthesis, Scene Understanding, efficient On-Device AI among other things. To support this we have a committee of diverse sets of expertise such as 3D computer vision, human visual perception, efficient ML, etc. </p>
        </div>
        <!-- ############################## section end -->

        <!-- ############################## section begin -->
        <div class="twelve columns" style="margin-bottom: 25px"  id="cfp">
          <div class="heading_dots_grey  hide-for-smal">
            <h1 class="heading_largesize" style="margin-bottom:0"><span class="heading_bg"> CALL FOR PAPERS </span></h1>
          </div>

          <div>
            <p style="text-align:center"><a class="success button" href="https://cmt3.research.microsoft.com/cv4mr2023">Submissions</a></p>
            <ul>
              <li>Real time View Synthesis for Passthrough</li>
              <li>Depth Estimation for Stereoscopic Reconstruction</li>
              <li>3D capture, reconstruction and rendering for virtual objects</li>
              <li>Scene understanding</li>
              <li>Real-time Style Transfer for Passthrough</li>
              <li>Novel Applications of Mixed Reality in areas such as Healthcare, Manufacturing, etc.</li>
            </ul>
          </div>
        <div>

        </div>
      </div>
        <!-- ############################## section end -->

        <!-- ############################## section begin -->
        <div class="twelve columns" style="margin-bottom: 25px"  id="speakers">
          <!-- ############################## speaker section begin -->
          <div class="heading_dots_grey  hide-for-smal">
            <h1 class="heading_largesize" style="margin-bottom:0"><span class="heading_bg"> KEYNOTE </span></h1>
          </div>
           <table>
            <tr> <!---#### Spreaker 1-->
              <td width=30%>
                <img src="speakers/richard.jpeg" width="400" height="500" style="margin-bottom: 25px">
              </td>
              <td>
                <p><b>Richard Newcombe</b> is VP of Research Science at Meta Reality Labs leading the Surreal team in Reality Labs Research. The Surreal team is creating a new generation of Machine Perception technologies called LiveMaps that combines novel always-on wearable sensing and compute with efficient algorithms for device location, 3D scene understanding and user state-estimation. The surreal team pioneered a new generation of machine perception glasses devices called project Aria that provides a new generation of data for ego-centric multimodal AI research.
              Richard received his undergraduate in Computer Science, and masters in Robotics and Intelligent Machines from the University of Essex in England, his PhD from Imperial College in London with a Postdoc at the University of Washington. Richard went on to co-found Surreal Vision, Ltd. that was acquired by Meta in 2015. As a research scientist his original work introduced the Dense SLAM paradigm demonstrated in KinectFusion and DynamicFusion that influenced a generation of real-time and interactive systems in AR/VR and robotics by enabling systems to efficiently understand the geometry of the environment.
              Richard received the best paper award at ISMAR 2011, best demo award ICCV 2011, best paper award at CVPR 2015 and best robotic vision paper award at ICRA 2017. In 2021, Richard received the ICCV Helmholtz award for research with DTAM, and the ISMAR and UIST test of time awards for KinectFusion. </p>
              </td>
            </tr>
          </table>

        <!-- ############################## section end -->

        <!-- ############################## section begin -->
        <div class="twelve columns" style="margin-bottom: 25px"  id="speakers">
          <div class="heading_dots_grey  hide-for-smal">
            <h1 class="heading_largesize" style="margin-bottom:0"><span class="heading_bg"> SPEAKERS </span></h1>
          </div>
          <!-- ############################## speaker section begin -->
           <table>
            <tr> <!---#### Spreaker 1-->
              <td width=30%>
                <img src="speakers/Anima-Anandkumar.jpg" width="400" height="500" style="margin-bottom: 25px">
              </td>
              <td>
                <p><b>Anima</b> s Bren Professor at Caltech and Senior Director of AI Research at NVIDIA. She received her B.Tech from the Indian Institute of Technology Madras, and her Ph.D. from Cornell University. She did her postdoctoral research at MIT and an assistant professorship at the University of California Irvine. She has received several honors such as the IEEE fellowship, Alfred. P. Sloan Fellowship, NSF Career Award, and Faculty Fellowships from Microsoft, Google, Facebook, and Adobe. She is part of the World Economic Forum's Expert Network. </p>
              </td>
            </tr>

            <tr> <!---#### Spreaker 1-->
              <td width=30%>
                <img src="speakers/angzoo.png" width="400" height="500" style="margin-bottom: 25px">
              </td>
              <td>
                <p><b>Angjoo Kanazawa</b> is an Assistant Professor in the Department of Electrical Engineering and Computer Sciences at the University of California, Berkeley. She leads the Kanazawa AI Research (KAIR) lab under BAIR and serves on the advisory board of Wonder Dynamics. She earned her BA in Mathematics and Computer Science from NYU working with Rob Fergus, and her PhD in Computer Science at the University of Maryland, College Park, where she was advised by David Jacobs. While in graduate school, she visited the Max Planck Institute in Tübingen, Germany, under the guidance of Michael Black. Before taking up her current teaching post, she worked as a Research Scientist at Google Research, and as a BAIR postdoc at UC Berkeley advised by Jitendra Malik, Alexei A. Efros and Trevor Darrell. Kanazawa's research lies at the intersection of computer vision, computer graphics, and machine learning. She is focused on building a system that can capture, perceive, and understand the complex ways that people and animals interact dynamically with the 3-D world--and can used that information to correctly identify the content of 2-D photos and video portraying scenes from everyday life. </p>
              </td>
            </tr>

            <tr> <!---#### Spreaker 1-->
              <td width=30%>
                <img src="speakers/natalia.jpeg" width="400" height="500" style="margin-bottom: 25px">
              </td>
              <td>
                <p><b>Natalia</b> is a Research Lead at Meta AI Research. Before coming to Meta, she completed her PhD at INSA Lyon and University of Guelph, advised by Christian Wolf and Graham Taylor. She also spent time as a visiting researcher at Google. Research interests: statistical machine learning and computer vision with emphasis on deep learning, 3D understanding and AR/VR applications. </p>
              </td>
            </tr>

            <tr> <!---#### Spreaker 1-->
              <td width=30%>
                <img src="speakers/rafal.jpg" width="400" height="500" style="margin-bottom: 25px">
              </td>
              <td>
                <p><b>Rafal</b> is a Professor of Graphics and Displays at the Department of Computer Science and Technology, the University of Cambridge in the United Kingdom. He received PhD from the Max-Planck Institute for Computer Science in Germany. His recent interests focus on computational displays, rendering and imaging algorithms that adapt to human visual performance and deliver the best image quality given limited resources, such as computation time or bandwidth. He contributed to early work on high dynamic range imaging, including quality metrics (HDR-VDP), video compression and tone-mapping. </p>
              </td>
            </tr>

            <tr> <!---#### Spreaker 1-->
              <td width=30%>
                <img src="speakers/ira.jpg" width="400" height="500" style="margin-bottom: 25px">
              </td>
              <td>
                <p><b>Ira Kemelmacher-Shlizerman's</b> research is in computer vision and computer graphics. Ira is an Associate Professor of Computer Science at the Allen School and Director of the UW Reality Lab. Prof. Kemelmacher-Shlizerman holds a PhD in Computer Science and Applied Mathematics from the Weizmann Institute of Science. Her works were awarded the Google faculty award, Madrona prize, the GeekWire Innovation of the Year Award, selected to the covers of CACM and SIGGRAPH, received the best student paper honorable mention at CVPR'21, and best demo runner up award MobiSys'22. Ira served as area chair and technical committee of most top conferences in both computer vision and graphics. She is a senior member of IEEE and was selected to Distinguished Member of ACM in 2021. In parallel to her academic career, Ira is currently leading an advanced tech team AR-ML for Commerce at Google. Previously Ira built a product Face Movies for Google in 2010; Founded a startup in 2016 which was acquired by Facebook/Meta; Spent 2 years at Facebook and built a video product. </p>
              </td>
            </tr>

          </table>

        <!-- ############################## section end -->

        <!-- ############################## section begin -->
        <div class="twelve columns" style="margin-bottom: 25px"  id="organizers">
          <!-- ############################## speaker section begin -->
          <div class="heading_dots_grey  hide-for-smal">
            <h1 class="heading_largesize" style="margin-bottom:0"><span class="heading_bg"> ORGANIZERS </span></h1>
          </div>
           <table>
            <tr> <!---#### Spreaker 1-->
              <td width=30%>
                <img src="speakers/rakesh.jpeg" width="400" height="500" style="margin-bottom: 25px">
              </td>
              <td>
                <p>Rakesh Ranjan is a Senior Research Scientist Manager in Reality Labs, Meta. Rakesh and his team pursue research in the areas of AI based low-level computer vision, 3D reconstruction and scene understanding for Augmented and Virtual Reality devices. Prior to Meta, Rakesh was a Research Scientist at Nvidia where he worked in AI for Real Time Graphics (DLSS) and AI for Cloud Gaming (GeForce Now). Rakesh also spent 5 years at Intel Research as a PhD and full-time researcher.</p>
              </td>
            </tr>
            <tr> <!---#### Spreaker 1-->
              <td width=30%>
                <img src="speakers/peter.jpg" width="400" height="500" style="margin-bottom: 25px">
              </td>
              <td>
                <p> Peter is a Research Manager in computer vision at Meta. Before joining Meta in 2014, he was Visiting Assistant Professor in Professor Bernd Girod’s group in Stanford University, Stanford, USA. he was working on personalized multimedia systems and mobile visual search. He received M.Sc. in Computer Science from the Vrije Universiteit, Amsterdam, Netherlands and a M.Sc. in Program Designer Mathematician from Eötvös Loránd University, Budapest, Hungary. Peter completed his Ph.D. with Prof. Touradj Ebrahimi at the Ecole Polytechnique Fédéral de Lausanne (EPFL), Lausanne, Switzerland, 2012.</p>
              </td>
            </tr>
            <tr> <!---#### Spreaker 1-->
              <td width=30%>
                <img src="speakers/lealtaixe.jpg" width="400" height="500" style="margin-bottom: 25px">
              </td>
              <td>
                <p> Laura Leal-Taixé is a Senior Research Manager at NVIDIA and also an Adjunct Professor at the Technical University of Munich (TUM), leading the Dynamic Vision and Learning group. From 2018 until 2022, she was a tenure-track professor at TUM. Before that, she spent two years as a postdoctoral researcher at ETH Zurich, Switzerland, and a year as a senior postdoctoral researcher in the Computer Vision Group at the Technical University in Munich. She obtained her PhD from the Leibniz University of Hannover in Germany, spending a year as a visiting scholar at the University of Michigan, Ann Arbor, USA. She pursued B.Sc. and M.Sc. in Telecommunications Engineering at the Technical University of Catalonia (UPC) in her native city of Barcelona. She went to Boston, USA to do her Masters Thesis at Northeastern University with a fellowship from the Vodafone foundation. She is a recipient of the Sofja Kovalevskaja Award of 1.65 million euros in 2017, the Google Faculty Award in 2021, and the ERC Starting Grant in 2022.</p>
              </td>
            </tr>
          </table>
        <!-- ############################## section end -->

      </div>
    </div>
    <div class="row">
      <div class="twelve columns">
        <ul id="menu3" class="footer_menu horizontal">
          <li><a href="index.html">Home</a></li>
        </ul>
        <script>$('ul#menu3').nav-bar(); </script>
      </div>
    </div>
  </div>
  <!-- end page wrap) -->
</div>
<!-- end page wrap) -->
<!-- Included JS Files (Compressed) -->
<script src="javascripts/foundation.min.js"></script>
<!-- Initialize JS Plugins -->
<script src="javascripts/app.js"></script>
</body>
</html>
