<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie9 lt-ie8" lang="en"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie9" lang="en"> <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
  <!--<![endif]-->
  <head>
    <title>CV4MR</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width" />
    <link rel="stylesheet" href="stylesheets/foundation.min.css" />
    <link rel="stylesheet" href="stylesheets/main.css" />
    <link rel="stylesheet" href="stylesheets/app.css" />
    <script src="javascripts/modernizr.foundation.js"></script>
    <!-- Google fonts -->
    <link
      href="http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300|Playfair+Display:400italic"
      rel="stylesheet"
      type="text/css"
    />
    <!-- IE Fix for HTML5 Tags -->
    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <h1 style="text-align: center; white-space: pre-wrap"><b>CV4MR</b></h1>
    <h2 style="text-align: center; white-space: pre-wrap">
      <b>Workshop on Computer Vision for Mixed Reality</b>
    </h2>
    <h3
      style="text-align: center; white-space: pre-wrap; color: rgb(87, 87, 94)"
    >
      June 18, 2023
    </h3>
    <h3
      style="text-align: center; white-space: pre-wrap; color: rgb(87, 87, 94)"
    >
      In conjunction with CVPR 2023
    </h3>
    <h3
      style="text-align: center; white-space: pre-wrap; color: rgb(87, 87, 94)"
    >
      Vancouver, Canada
    </h3>
    <p style="text-align: center; white-space: pre-wrap" class=""></p>

    <div class="row page_wrap" style="margin-top: -2px">
      <!-- page wrap -->
      <div class="twelve columns">
        <!-- page wrap -->
        <div class="row">
          <div
            class="twelve columns header_nav"
            style="margin-bottom: 0; box-shadow: none"
          >
            <div class="twelve columns">
              <ul id="menu-header" class="nav-bar horizontal">
                <li><a href="index.html">Home</a></li>
                <li><a href="index.html#overview">Overview</a></li>
                <li><a href="index.html#cfp">Call for Papers</a></li>
                <li><a href="index.html#speakers">Speakers</a></li>
                <li><a href="program.html">Schedule</a></li>
                <li><a href="index.html#organizers">Organizers</a></li>
              </ul>
              <script>
                $("ul#menu-header").nav - bar();
              </script>
            </div>
          </div>
        </div>
        <!-- END Header -->
        <div class="row">
          <div class="blog_post">
            <!-- Begin Blog Post -->
            <!-- ############################## section begin -->
            <div
              class="twelve columns"
              style="margin-bottom: 25px"
              id="program"
            >
              <div class="heading_dots_grey hide-for-small">
                <h1 class="heading_mediumsize" style="margin-bottom: 0">
                  <span class="heading_bg"> Schedule </span>
                </h1>
                Zoom link: TBA
              </div>
            </div>
            <div>
              <table style="width: 100%; border-collapse: collapse">
                <tr>
                  <td>08:30-09:00 am</td>
                  <td>Rakesh Ranjan</td>
                  <td><b>Opening Remarks</b></td>
                </tr>
                <tr>
                  <td>09:00-09:30 am</td>
                  <td><a href="https://scholar.google.co.uk/citations?user=MhowvPkAAAAJ">Richard Newcombe</a></td>
                  <td style="border: none; max-width: 650px">
                    <b
                      >Project Aria: building machine perception that works
                      everywhere for XR and Contextualized AI</b
                    >
                    <br />
                    The last decade saw rapid progress in computer vision,
                    natural language processing and more generally AI, built on
                    a foundation of machine learning based techniques fed with
                    internet scale textual, image and video datasets from web
                    era content. In this talk we'll introduce the gap that
                    exists between those datasets and resulting machine
                    perception algorithms, and the solutions we'll need to
                    enable a fully contextually aware, always-on, class of AI
                    that understands the world in real time in physical reality.
                    We'll look at key problems to be solved in machine
                    perception and opportunities for research with the new
                    generation of egocentric sensory data from Project Aria,
                    providing a view of what will become possible with a new
                    generation of glasses form-factor wearable computers.
                  </td>
                </tr>
                <tr>
                  <td>09:30-10:00 am</td>
                  <td><a href="https://ai.stanford.edu/~dahuang/"> De-An Huang</a></td>
                  <td style="border: none; max-width: 650px">
                    <b>Annotation Efficient Scene Understanding</b>
                  <br>
                  Scene understanding is one of the key building blocks of Mixed Reality. By segmenting and tracking objects and backgrounds in a scene, one can better augment the scene with virtual effects. However, scene understanding models are often trained with costly pixel-level annotations. In this talk, I will discuss our recent research on reducing the annotation costs for scene understanding: (1) We show that object instance segmentation naturally emerges from self-supervised pre-training, and just adding box supervision retains up to 97.4% of the performance of full mask supervision. (2) We further show that a class of image instance segmentation is temporally consistent and can be used to track objects in videos without any tracking annotation. (3) When annotation is needed for scene understanding, we demonstrate the effectiveness of point supervision, which is two orders of magnitude more efficient than full mask annotation.
                  </td>
                </tr>
                <tr>
                  <td>10:00-10:30 am</td>
                  <td><a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a></td>
                  <td></td>
                </tr>
                <tr>
                  <td>10:30-11:00 am</td>
                  <td>Poster Spotlight + Break</td>
                  <td></td>
                </tr>
                <tr>
                  <td>11:00-11:30 am</td>
                  <td><a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a></td>
                  <td style="border: none; max-width: 650px">
                    <b
                      >Interacting with the 3D World via Language</b
                    >
                    <br />
                    Tools like <a href="https://docs.nerf.studio/en/latest/">nerf.studio</a> enable practical capture of 3D worlds, but how are we going to interact with the digitized worlds? In this talk I will first talk about nerf.studio, a modular open-source framework for easily creating photorealistic 3D scenes and accelerating NeRF development. I will talk about our goals, insights, and new features, such as the VFX integration. I will then talk about Language Embedded Radiance Fields (LERFs), which enables users to interact with NeRF scenes with flexible natural language. It distills CLIP features in 3D space, which enables it to read text, as well as find long-tail, abstract objects, and semantics at multiple-scales. 
                  </td>
                </tr>
                <tr>
                  <td>11:30-12:00 pm</td>
                  <td><a href="https://nneverova.github.io/">Natalia Neverova</a></td>
                  <td></td>
                </tr>
                <tr>
                  <td>12:00-12:30 pm</td>
                  <td><a href="https://www.cl.cam.ac.uk/~rkm38/">Rafal Mantiuk</a></td>
                  <td style="border: none; max-width: 650px">
                    <b
                      >Reproducing reality on an ultra-realistic 3D HDR display</b
                    >
                    <br />
                    It is possible to capture and reproduce on a display
real-world scenes that are indistinguishable from their real
counterparts - the scenes that pass the visual Turing test. To achieve
this goal, we built a custom high-dynamic-range multi-focal stereo
display capable of accurate reproduction of colour and most of the
depth cues. We captured small scenes, 60-80cm from the camera, as HDR
light fields, and then rendered those on our display as lumigraphs. In
this talk I will discuss the challenges of geometric and colour
calibration of cameras and displays, rendering from multiple 40 MPixel
HDR images and subjective testing that let us prove the accuracy of
our capture-and-display system.
                  </td>
                </tr>
              </table>
            </div>
            <!-- ############################## section end -->
          </div>
        </div>

        <div class="row">
          <div class="twelve columns">
            <ul id="menu3" class="footer_menu horizontal">
              <ul>Contact: <a href="mailto:cv4mr@googlegroups.com">cv4mr@googlegroups.com</a></ul>
              <ul><a href="index.html">Home</a></ul>
            </ul>
            <script>$('ul#menu3').nav-bar(); </script>
          </div>
        </div>
      </div>
      <!-- end page wrap) -->
    </div>
    <!-- end page wrap) -->
    <!-- Included JS Files (Compressed) -->
    <script src="javascripts/foundation.min.js"></script>
    <!-- Initialize JS Plugins -->
    <script src="javascripts/app.js"></script>
  </body>
</html>
